{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: backpropagation and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. .grad\n",
    "2. .backword()\n",
    "3. retain_grad(),retain_graph\n",
    "4. torch.optim, .stop()\n",
    "\n",
    "Examples:\n",
    "1. scaler example\n",
    "2. vector example\n",
    "3. function example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch introduction\n",
    "\n",
    "Pytorch python library that provided tools to work with tensors.\n",
    "Feature: allows for Trcking gradient odf tensors\n",
    "\n",
    "Tensors: a multidimentional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor\n",
    "\n",
    "my_tensor = torch.tensor([[1.0,2.0],[1.0,2.0]])\n",
    "my_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple scalar example\n",
    " \n",
    "Let's define the following: $\\\\ p \\in \\mathbb{R} \\\\ w = 10p \\\\ l = w^2$.\n",
    " \n",
    "From calculus we know that:\n",
    "$$  \\frac{\\partial w}{\\partial p} = 10, \\\n",
    "\\frac{\\partial l}{\\partial w} = 2w, \\\n",
    "\\frac{\\partial l}{\\partial p} = \\frac{\\partial l}{\\partial w} \\frac{\\partial w}{\\partial p} = 2w * 10 = 2(10p)*10 = 200p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that p is @free@ (it doesn't depend on anything)\n",
    "\n",
    "l -- > w --> p\n",
    "\n",
    "Actually, torch is building this kind of graph when I'll define thoes tensors. This graph that toech build is called computational graph.\n",
    "Tensor that do not depend in anything (p) are called leaf node (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True) tensor([10.], grad_fn=<MulBackward0>) tensor([100.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def init_variables(scaler: float=1.0, requires_grad: bool=True):\n",
    "    p = torch.tensor([scaler],requires_grad=requires_grad)\n",
    "    w = 10*p \n",
    "    l= w**2 # pow(2)\n",
    "    return p, w, l\n",
    "\n",
    "p,w,l=init_variables(1.0)\n",
    "print(p,w,l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each torch has a requires_grid attribute. this attr allows for tracking gradients for the tensor --> if a tensor has requrs_grad=True, it'll be attached to computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s0 ...how to compute derivatives??\n",
    "\n",
    "In torch, each tensor having requires_grad=True, has a method called .backwards(), which computes the derivative of that tensor wrt the leaf nodes.\n",
    "\n",
    "tensor.backwords()\n",
    "\n",
    "w.backwords()\n",
    "\n",
    "## Graph leaves are alwayes the tensors will compute the gradients with respect to when calling BACKWARD()\n",
    "\n",
    "bUT WHERE THE RESULT IS SORTED?\n",
    "in torch, when yo call .backward(), the result of computational is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,w,_=init_variables(scaler=2.0)\n",
    "p.backward()\n",
    "p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18869/406291535.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "p,w,_=init_variables(scaler=2.0)\n",
    "w.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18869/2209070905.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "def init_variables_retain_w(scaler: float=1.0, requires_grad: bool=True):\n",
    "    p = torch.tensor([scaler],requires_grad=requires_grad)\n",
    "    w = 10*p \n",
    "    l= w**2 # pow(2)\n",
    "    return p, w, l\n",
    "p,w,l=init_variables_retain_w(1.0)\n",
    "w.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "p,w,l=init_variables(scaler=1.0)\n",
    "l.backward()\n",
    "\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl / dw\n",
    "p ,w, l = init_variables_retain_w(scaler=1.0)\n",
    "l.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# dl / dp =  dl /dw * dw / dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "p ,w, l = init_variables_retain_w(scaler=1.0)\n",
    "w.backward(retain_graph=True)\n",
    "print(p.grad)\n",
    "# computational graph doesn't exist anymore.\n",
    "#p.gard = torch.zeros_like(p)\n",
    "#p.grad = torch.zero([1])\n",
    "p.grad.zero_()\n",
    "w.backward()\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clone and deattach\n",
    "\n",
    "clone: makes a copy of the tensor it is called on ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with tensors\n",
    " \n",
    "Let's define the following: $\\\\ p = (2,2,2,2) \\\\ w = p^2$ \\\n",
    "and a  function $$ \\ell(p,w) = \\sum_{i=1}^{4} (p_i - w_i)^2 = \\sum_{i=1}^{4} (p_i - p_i^2)^2 = \\ell(p). $$\n",
    "Hence $$ \\frac{\\partial \\ell}{\\partial p_i} = 2(p_i-p_i^2)(1-2p_i) = 4p_i^3-6p_i^2+2p_i $$\n",
    " \n",
    "In particular:\n",
    "$$ \\nabla_p \\ell = (\\frac{\\partial \\ell}{\\partial p_1}, \\frac{\\partial \\ell}{\\partial p_2}, \\frac{\\partial \\ell}{\\partial p_3}, \\frac{\\partial \\ell}{\\partial p_4}) = (4p_1^3-6p_1^2+2p_1, 4p_2^3-6p_2^2+2p_2, 4p_3^3-6p_3^2+2p_3, 4p_4^3-6p_4^2+2p_4)$$\n",
    " \n",
    "Note also taht:\n",
    "$$ \\frac{\\partial \\ell}{\\partial w_i} = -2(p_i-w_i) = -2(p_i-p_i^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2.], requires_grad=True) tensor([4., 4., 4., 4.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "p = torch.tensor([2.0,2.0,2.0,2.0],requires_grad=True)\n",
    "w = p**2\n",
    "print(p,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<SumBackward0>)\n",
      "tensor([12., 12., 12., 12.])\n"
     ]
    }
   ],
   "source": [
    "def l_fn(p,w) -> torch.Tensor:\n",
    "    return (p-w).pow(2).sum()\n",
    "\n",
    "p= torch.tensor([2.0,2.0,2.0,2.0],requires_grad=True)\n",
    "w = p**2\n",
    "l = l_fn(p,w)\n",
    "print(l)\n",
    "l.backward()  # dl / dp, result is 2*(p-w) = 2*(2-4) = -4\n",
    "\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Far...\n",
    "\n",
    "1. we can compute derivatives of scaler (l) wrt to leaf nodes (p) (l.backward())\n",
    "2. the result will be in p.grad\n",
    "\n",
    "Next step: how to change p according to p.grad?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the previous example, changing a little bit numbers.\n",
    " \n",
    "Let's define the following: $\\\\ p = (1,3,2,2) \\\\ w = p^2$\n",
    "$$ \\ell(p,w) = \\sum_{i=1}^{4} (p_i - w_i)^2 = \\sum_{i=1}^{4} (p_i - p_i^2)^2 = \\ell(p). $$\n",
    "We already know that $$ \\frac{\\partial \\ell}{\\partial p_i} = 2(p_i-p_i^2)(1-2p_i) = 4p_i^3-6p_i^2+2p_i $$\n",
    "and we know that the content of `p.grad` is simply:\n",
    "$$ \\nabla_p \\ell = (\\frac{\\partial \\ell}{\\partial p_1}, \\frac{\\partial \\ell}{\\partial p_2}, \\frac{\\partial \\ell}{\\partial p_3}, \\frac{\\partial \\ell}{\\partial p_4}) = (4p_1^3-6p_1^2+2p_1, 4p_2^3-6p_2^2+2p_2, 4p_3^3-6p_3^2+2p_3, 4p_4^3-6p_4^2+2p_4)$$\n",
    "which in our case is:\n",
    "$$p.grad = \\nabla_p \\ell = (4-6+2, 108-54+6, 32-24+4, 32-24+4) = (0, 60, 12, 12)$$\n",
    " \n",
    "When we call `optimizer.step()` with learning rate $\\eta$ at time $t$, we get:\n",
    "$$p_{i, next} = p_{i,before} - \\eta * \\frac{\\partial \\ell}{\\partial p_i} = p_{i,before} - \\eta * (4p_i^3-6p_i^2+2p_i)$$\n",
    " \n",
    "Putting numbers together, we get:\n",
    "$$p_{1, next} = 1 - \\eta * 0 = 1, \\\\\n",
    "p_{2, next} = 3 - \\eta * 60 = 3 - 60\\eta, \\\\\n",
    "p_{3, next} = 2 - \\eta * 12 = 2 - 12\\eta, \\\\\n",
    "p_{4, next} = 2 - \\eta * 12 = 2 - 12\\eta.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_fn(p,w) -> torch.Tensor:\n",
    "    return (p-w).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p beforetensor([2., 2., 2., 2.], requires_grad=True)\n",
      "p after tensor([-10., -10., -10., -10.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "p = torch.tensor([2.0,2.0,2.0,2.0],requires_grad=True)\n",
    "w = p**2\n",
    "print(f'p before{p}')\n",
    "\n",
    "opt = torch.optim.SGD([p],lr=1.0)\n",
    "\n",
    "l = l_fn(p,w)\n",
    "\n",
    "opt.zero_grad()\n",
    "l.backward()\n",
    "opt.step()\n",
    "print(f'p after {p}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we can compute gradients dl/dp\n",
    "2. we can update p according to dl/dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor NN\n",
    "\n",
    "f(input,p)--> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4963, 0.7682], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def a_funcion (input_tensor: torch.Tensor , p: torch.Tensor) -> torch.Tensor:\n",
    "    return input_tensor*p\n",
    "dim = 2\n",
    "torch.manual_seed(0)\n",
    "p = torch.rand([2],requires_grad=True)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.rand([100,2])\n",
    "dataset.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRUTIAL POINT**\n",
    "1. Define a task: learn how to multiply by 2 the input_tensor\n",
    "2. define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    input : Will be a datapoint from the dataset\n",
    "    output: a_function(input, p) \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    tmp =  (output-2.*input).pow(2).mean(-1)\n",
    "    print(tmp.shape)\n",
    "    return tmp.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial p: tensor([0.4963, 0.7682], requires_grad=True)\n",
      "tensor([0.2586, 0.6317], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2607, 0.6386], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2628, 0.6454], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2649, 0.6521], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2670, 0.6588], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2691, 0.6655], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2712, 0.6721], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2733, 0.6787], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2754, 0.6852], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2775, 0.6917], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2796, 0.6982], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2816, 0.7046], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2837, 0.7109], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2858, 0.7172], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2878, 0.7235], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2899, 0.7297], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2919, 0.7359], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2940, 0.7421], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2960, 0.7482], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.2980, 0.7542], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3000, 0.7602], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3020, 0.7662], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3041, 0.7722], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3061, 0.7781], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3081, 0.7839], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3100, 0.7897], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3120, 0.7955], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3140, 0.8013], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3160, 0.8070], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3180, 0.8126], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3199, 0.8183], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3219, 0.8239], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3238, 0.8294], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3258, 0.8349], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3277, 0.8404], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3297, 0.8458], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3316, 0.8512], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3335, 0.8566], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3355, 0.8619], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3374, 0.8672], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3393, 0.8725], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3412, 0.8777], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3431, 0.8829], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3450, 0.8880], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3469, 0.8931], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3488, 0.8982], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3507, 0.9033], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3525, 0.9083], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3544, 0.9133], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3563, 0.9182], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3581, 0.9231], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3600, 0.9280], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3619, 0.9328], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3637, 0.9377], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3655, 0.9424], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3674, 0.9472], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3692, 0.9519], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3710, 0.9566], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3729, 0.9612], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3747, 0.9659], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3765, 0.9705], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3783, 0.9750], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3801, 0.9795], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3819, 0.9840], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3837, 0.9885], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3855, 0.9929], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3873, 0.9973], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3890, 1.0017], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3908, 1.0061], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3926, 1.0104], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3943, 1.0147], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3961, 1.0189], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3978, 1.0232], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.3996, 1.0274], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4013, 1.0315], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4031, 1.0357], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4048, 1.0398], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4065, 1.0439], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4083, 1.0480], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4100, 1.0520], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4117, 1.0560], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4134, 1.0600], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4151, 1.0639], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4168, 1.0679], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4185, 1.0718], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4202, 1.0756], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4219, 1.0795], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4236, 1.0833], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4253, 1.0871], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4269, 1.0909], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4286, 1.0946], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4303, 1.0983], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4319, 1.1020], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4336, 1.1057], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4352, 1.1093], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4369, 1.1130], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4385, 1.1166], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4402, 1.1201], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4418, 1.1237], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "tensor([0.4434, 1.1272], grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.8542, 1.3750], requires_grad=True)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.rand([dim],requires_grad=True)\n",
    "print(f'initial p: {p}')\n",
    "dataset = torch.rand([100,dim])\n",
    "\n",
    "opt = torch.optim.SGD([p],lr=0.01)\n",
    "updates = 0\n",
    "for data in dataset:\n",
    "    opt.zero_grad()\n",
    "    output = a_funcion(data,p)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "    l = loss(data,output)\n",
    "    opt.zero_grad()\n",
    "    l.backward() # dloss / dp\n",
    "    opt.step()\n",
    "    updates +=1\n",
    "print(f'initial p: {p}')\n",
    "print(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial p: tensor([0.4963, 0.7682], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.4974, 0.7704], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.5116, 0.8198], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.5474, 0.9146], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.5775, 0.9580], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.5949, 0.9749], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.5949, 0.9778], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.6071, 1.0053], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.6749, 1.0689], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.6783, 1.0763], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.7397, 1.1537], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.7596, 1.2184], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.7814, 1.2423], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.8920, 1.2424], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.8958, 1.2529], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.9061, 1.3178], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.9095, 1.3228], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.9119, 1.3229], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.9167, 1.3814], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([0.9733, 1.4155], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.0017, 1.4190], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.0359, 1.4190], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.0377, 1.4224], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.1017, 1.4588], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.1087, 1.4713], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.1686, 1.5239], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.2091, 1.5392], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.2643, 1.5412], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.2902, 1.5418], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.2919, 1.5444], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.3292, 1.5668], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.3320, 1.5852], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.3721, 1.5931], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.3890, 1.6085], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.4291, 1.6461], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.4298, 1.6497], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.4575, 1.6790], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5049, 1.7074], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5227, 1.7075], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5370, 1.7086], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5370, 1.7345], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5729, 1.7345], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5879, 1.7391], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.5951, 1.7411], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6145, 1.7421], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6325, 1.7567], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6596, 1.7682], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6596, 1.7689], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6787, 1.7774], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.6791, 1.7784], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7093, 1.7939], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7116, 1.7968], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7116, 1.8017], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7121, 1.8020], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7185, 1.8085], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7210, 1.8207], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7220, 1.8370], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7418, 1.8371], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7454, 1.8415], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7538, 1.8476], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7657, 1.8519], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7672, 1.8599], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7672, 1.8605], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7705, 1.8614], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7729, 1.8615], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7765, 1.8666], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7771, 1.8696], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7935, 1.8722], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.7990, 1.8749], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8063, 1.8833], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8246, 1.8911], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8413, 1.8934], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8413, 1.8942], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8525, 1.8968], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8535, 1.8969], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8535, 1.8970], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8558, 1.9031], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8644, 1.9031], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8733, 1.9033], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8753, 1.9041], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8773, 1.9057], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8773, 1.9057], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8795, 1.9081], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8804, 1.9125], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8804, 1.9144], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.8910, 1.9151], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9009, 1.9191], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9009, 1.9245], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9028, 1.9250], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9107, 1.9251], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9134, 1.9263], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9198, 1.9293], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9242, 1.9325], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9253, 1.9336], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9253, 1.9375], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9314, 1.9420], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9315, 1.9435], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9316, 1.9438], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9319, 1.9464], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9322, 1.9476], requires_grad=True)\n",
      "torch.Size([])\n",
      "initial p: tensor([1.9341, 1.9512], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "p = torch.rand([dim],requires_grad=True)\n",
    "print(f'initial p: {p}')\n",
    "dataset = torch.rand([100,dim])\n",
    "lr = 0.1\n",
    "#opt = torch.optim.SGD([p],lr=0.01)\n",
    "\n",
    "for data in dataset:\n",
    "    output = a_funcion(data,p)\n",
    "\n",
    "\n",
    "    l = loss(data,output)\n",
    "    p.grad = torch.zeros_like(p)\n",
    "\n",
    "    l.backward() # dloss / dp\n",
    "    with torch.no_grad():\n",
    "        p -= lr*p.grad\n",
    "    \n",
    "\n",
    "    #p_grad = p.grad\n",
    "    #p = p.detach()\n",
    "    #p = p - lr*p\n",
    "    #p.requires_grad = True\n",
    "\n",
    "\n",
    "    print (f'initial p: {p}')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
