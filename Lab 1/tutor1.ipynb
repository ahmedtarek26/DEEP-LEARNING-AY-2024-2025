{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and backpropagtion in `PyTorch` (by Michele Alessi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TakeawaysðŸ˜‡:\n",
    "1. Understand `.grad` attribute\n",
    "2. Understand `.backward()` method\n",
    "3. Deal with advanced features like `retain_graph` and `retain_grad()`\n",
    "4. 1 + 2 *should* introduce you smoothly to `optimizers` and `.step()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the following examples:\n",
    "1. a scalar (ie dealing with numbers) example to understand how gradients are calculated and backpropagated.\n",
    "2. a vector (ie dealing with tensors) example.\n",
    "3. an example involving functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch introduction\n",
    "`PyTorch` is a library that basically provides tools to work with tensors. Tensors are nothing but multi-dimensional arrays.\n",
    "\n",
    "We have many operations that we can do with tensors, like addition, multiplication, etc. Many of them work similarly to numpy.\n",
    "\n",
    "Other operations are specific to `torch`, but we will see them as we go on (I think it is better if we analyze an operation in the proper context rather then just list them).\n",
    "\n",
    "To start, the basic operation we need is how to define a tensor. We can do it with the `torch.tensor()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(my_tensor)\n",
    "print(my_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big thing about Pytorch is that it provides automatic differentiation. This means that we can calculate the gradients of a function with respect to its parameters without having to do it manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple scalar example\n",
    "\n",
    "Let's define the following: $\\\\ p \\in \\mathbb{R} \\\\ w = 10p \\\\ l = w^2$. \n",
    "\n",
    "From calculus we know that:\n",
    "$$  \\frac{\\partial w}{\\partial p} = 10, \\\n",
    "\\frac{\\partial l}{\\partial w} = 2w, \\\n",
    "\\frac{\\partial l}{\\partial p} = \\frac{\\partial l}{\\partial w} \\frac{\\partial w}{\\partial p} = 2w * 10 = 2(10p)*10 = 200p $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that p is 'free', it does not depend on anything. \n",
    "w depends on p, and l depends on w, which in turn depends on p.\n",
    "\n",
    "l --> w --> p\n",
    "\n",
    "We can see this as a graph. Actually torch works with this kind of graph, it is called the computational graph.\n",
    "\n",
    "The tensors that do not depend on anything are called leaf tensors. \n",
    "\n",
    "In our example p is the only leaf tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([1.], requires_grad=True)\n",
      "w = 10p: tensor([10.], grad_fn=<MulBackward0>)\n",
      "l = w^2: tensor([100.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def init_variables(scalar: float = 1.0, requires_grad: bool = True) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    p = torch.tensor([scalar], requires_grad=requires_grad)\n",
    "    w = 10*p\n",
    "    l = w**2\n",
    "    return p, w, l\n",
    "\n",
    "p, w, l = init_variables(scalar=1.0)\n",
    "print('p:', p)\n",
    "print('w = 10p:', w)\n",
    "print('l = w^2:', l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `torch` tensor has a `requires_grad` attribute that allows for tracking the gradients with respect to that tensor (in other words, if reqiures_grad=True, the tensor will be attached to the computational graph). \\\n",
    "If `requires_grad=True`, then we are able to compute the gradients with respect to that tensor.\n",
    "\n",
    "To be more specific about the definition of a leaf node, a leaf node is a tensor that does not depend on any other (ie it is not the results of any operation) and it has `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([1.])\n",
      "p.requires_grad:  False\n",
      "p:  tensor([1.], requires_grad=True)\n",
      "p.requires_grad:  True\n"
     ]
    }
   ],
   "source": [
    "p, _, _ = init_variables(scalar=1.0, requires_grad=False)\n",
    "print('p:', p)\n",
    "print('p.requires_grad: ', p.requires_grad)\n",
    "p.requires_grad = True\n",
    "print('p: ', p)\n",
    "print('p.requires_grad: ', p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... how to compute gradients with pytorch? Using the `.backward()` method!\n",
    "\n",
    "`backward()` is a method of each tensor that calculates the gradient of the tensor it is called on wrt the graph leaves.\n",
    "\n",
    "w.backward() ---> dw/dp \n",
    "\n",
    "**Graph leaves are ALWAYS the tensors that we will compute the gradients with respect to when calling .backward().**\n",
    " \n",
    "Ok but then... where do we see the gradients???\n",
    "\n",
    "The gradients are stored in the `.grad` attribute of the tensor we computed the gradient with respect to. (so we would always like to access the `.grad` attribute of the leaf tensor).\n",
    "\n",
    "w.backward() compute dw/dp and we see the result in p.grad\n",
    "\n",
    "So to go on: Let's say we want to compute the gradient of `w` with respect to `p`. \n",
    "\n",
    "We would call `w.backward()` and then the gradient would be stored in `p.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:  tensor([1.], requires_grad=True)\n",
      "w:  tensor([10.], grad_fn=<MulBackward0>)\n",
      "p.requires_grad:  True\n",
      "w.requires_grad:  True\n",
      "dw/dp = p.grad:  tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "\n",
    "print('p: ',p)\n",
    "print('w: ',w)\n",
    "print('p.requires_grad: ',p.requires_grad)\n",
    "print('w.requires_grad: ',w.requires_grad)\n",
    "\n",
    "w.backward()\n",
    "print('dw/dp = p.grad: ',p.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE FOR LATER**: .backward() has to be called on a scalar tensor. \n",
    "\n",
    "In this case all is scalar, but later we will see that we have to be careful with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen if I try to compute the gradient of `p` with respect to `p`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp/dp = p.grad:  tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "p, *_ = init_variables(scalar=1.0, requires_grad=True)\n",
    "p.backward()\n",
    "print('dp/dp = p.grad: ',p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen if I compute `dw/dp` (like before) and try to access `w.grad`? (Note that w IS NOT a leaf tensor).\n",
    "\n",
    "We know that dw/dp (=w.backward() note again that call w.backward() is telling pytorch: do the derivative of w wrt the leaf node) is the grad of w wrt p, and that the results is stored in p.grad.\n",
    "\n",
    "\n",
    "We might expect that accessing w.grad would show the result of dw/dw (which we are not directly computing, but which is computed for the chain rule internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw/dp = p.grad:  tensor([10.])\n",
      "dw/dw = w.grad:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/r059lv3n7nq9x6gh78v25xk80000gn/T/ipykernel_62126/1943631255.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print('dw/dw = w.grad: ',w.grad)\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "w.backward()\n",
    "print('dw/dp = p.grad: ',p.grad)\n",
    "print('dw/dw = w.grad: ',w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the `.grad` attribute is \"natively\" accessible only for leaf tensors (ie that attributes normally is populated only for leaf tensors).\n",
    "\n",
    "If we want to access the gradient of `w`, we need to call `retain_grad()` method on `w` before calling any `backward()`. This way, when calling backward, the computational graph is populated also for the non-leaf tensor w.\n",
    "Let's modify the `init_params()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables_retain_w(scalar: float = 1.0, requires_grad: bool = True) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    p = torch.tensor([scalar], requires_grad=requires_grad)\n",
    "    w = 10*p\n",
    "    w.retain_grad() # keep the gradient for w, which is not a leaf tensor since it depends on p\n",
    "    l = w**2\n",
    "    return p, w, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw/dp = p.grad:  tensor([10.])\n",
      "dw/dw = w.grad:  tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables_retain_w(scalar=1.0, requires_grad=True)\n",
    "w.backward()\n",
    "print('dw/dp = p.grad: ',p.grad)\n",
    "print('dw/dw = w.grad: ',w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work also with l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:  tensor([1.], requires_grad=True) w:  tensor([10.], grad_fn=<MulBackward0>) l:  tensor([100.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p, w, l = init_variables(scalar=1.0, requires_grad=True)\n",
    "print('p: ',p, 'w: ',w, 'l: ',l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you call backward on l, this is equivalent to which operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl/dp = p.grad:  tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "print('dl/dp = p.grad: ',p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful with accumulation!!!\n",
    "So far so good... but... \\\n",
    "**Note**: The `backward()` method ALWAYS accumulates the gradients in the `.grad` attribute of the leaves. \\\n",
    "This means that if we call `backward()` multiple times, the gradients will be accumulated in the `.grad` attribute. \\\n",
    "Let's try to call `backward()` 2 times on `w` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m p, w, l \u001b[38;5;241m=\u001b[39m init_variables(scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# w --> p\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# show that it doesnt depend on the order or anything!\u001b[39;00m\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "p, w, l = init_variables(scalar=1.0, requires_grad=True)\n",
    "l.backward()\n",
    "w.backward()\n",
    "# w --> p\n",
    "# show that it doesnt depend on the order or anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch frees computational graphs after a backward pass by default!! \n",
    "This means that after the closure of the first call of .backward(), the computational graph doesn't exists anymore!!!\n",
    "\n",
    "If we want to call .backward() multiple times, we must keep the computational graph by passing retain_graph=True.\n",
    "\n",
    "This tells PyTorch: \"Don't free the graph after the first backward pass.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw/dp = p.grad:  tensor([10.])\n",
      "dw/dp = p.grad:  tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "p, w, l = init_variables(scalar=1.0, requires_grad=True)\n",
    "w.backward(retain_graph=True)\n",
    "print('dw/dp = p.grad: ',p.grad) # add 10 to p.grad\n",
    "w.backward()\n",
    "print('dw/dp = p.grad: ',p.grad) # add 10 again to p.grad: results=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we see an error! This occurs because we are accumulating gradients wrongly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to zero the gradient to get the correct result. \\\n",
    "If we want to zero out the gradients, we can call the `zero_grad()` method on the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw/dp = p.grad:  tensor([10.])\n",
      "dw/dp = p.grad:  tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "w.backward(retain_graph=True)\n",
    "print('dw/dp = p.grad: ',p.grad)\n",
    "p.grad.zero_()\n",
    "#p.grad = None\n",
    "#p.grad = torch.tensor([0.0])\n",
    "w.backward()\n",
    "print('dw/dp = p.grad: ',p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in general you will NEVER be in a situation like this.\n",
    "\n",
    "What happen typically is that:\n",
    "1. you have some parameters (the leaves, our p)\n",
    "2. you do some intermediate operations (but you \"don't see\" these operations when computing gradients!) like our w\n",
    "3. you end up with one number (our l, which will be the loss), and you backpropagate on the loss (you see this? l.backward() is exactley dl/dp, the derivative of the loss wrt the parameters ðŸ˜‡)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini exercise: extract dl/dw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, w, l = init_variables_retain_w(scalar=3.2)\n",
    "l.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.detach()` and `.clone()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see 2 important opertions.\n",
    "\n",
    "`detach()`, which is used to detach a tensor from the computational graph. This means that the tensor will not require gradients anymore. Also, if a tensor has a .grad attribute, it will lose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  tensor([10.], grad_fn=<MulBackward0>)\n",
      "w:  tensor([10.])\n",
      "p:  tensor([1.], requires_grad=True)\n",
      "w:  tensor([10.], requires_grad=True)\n",
      "p.requires_grad:  True\n",
      "w.requires_grad:  True\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "print('w: ',w)\n",
    "w = w.detach()\n",
    "print('w: ',w)\n",
    "w.requires_grad = True\n",
    "print('p: ',p)\n",
    "print('w: ',w)\n",
    "print('p.requires_grad: ',p.requires_grad)\n",
    "print('w.requires_grad: ',w.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "w.backward()\n",
    "print(p.grad)\n",
    "p = p.detach()\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clone()` is used to clone a tensor. The new tensor has same data but is stored in a **different** memory location.\n",
    "\n",
    "If the original tensor has `requires_grad=True`, the clone will also require gradients and be tracked by autograd.\n",
    "\n",
    "Changes to the cloned tensor do not modify the original tensor and vice versa.\n",
    "\n",
    "The clone doesn't inherit the `.grad` attribute of the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "None\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/r059lv3n7nq9x6gh78v25xk80000gn/T/ipykernel_62126/3497687827.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(p_clone.grad)\n"
     ]
    }
   ],
   "source": [
    "p, w, _ = init_variables(scalar=1.0, requires_grad=True)\n",
    "w.backward()\n",
    "p_clone = p.clone()\n",
    "print(p.grad)\n",
    "print(p_clone.grad)\n",
    "print(p.requires_grad)\n",
    "print(p_clone.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far should be ok:\n",
    "0. `.requires_grad`: attach a tensor to the computational graph (ie allow for automaitc computation of derivatives)\n",
    "1. `tensor.backward()`: computes d tensor/ d leaf node\n",
    "2. `leaf.grad`: stores the results of calling backward(), ie stores d tensor/ d leaf node\n",
    "3. `retain_graph` and `tensor.retain_grad()`: advanced methods to do stuff that probabily you will never have to do\n",
    "4. How to zero the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with tensors\n",
    "\n",
    "Let's define the following: $\\\\ p = (2,2,2,2) \\\\ w = p^2$ \\\n",
    "and a  function $$ \\ell(p,w) = \\sum_{i=1}^{4} (p_i - w_i)^2 = \\sum_{i=1}^{4} (p_i - p_i^2)^2 = \\ell(p). $$\n",
    "Hence $$ \\frac{\\partial \\ell}{\\partial p_i} = 2(p_i-p_i^2)(1-2p_i) = 4p_i^3-6p_i^2+2p_i $$\n",
    "\n",
    "In particular:\n",
    "$$ \\nabla_p \\ell = (\\frac{\\partial \\ell}{\\partial p_1}, \\frac{\\partial \\ell}{\\partial p_2}, \\frac{\\partial \\ell}{\\partial p_3}, \\frac{\\partial \\ell}{\\partial p_4}) = (4p_1^3-6p_1^2+2p_1, 4p_2^3-6p_2^2+2p_2, 4p_3^3-6p_3^2+2p_3, 4p_4^3-6p_4^2+2p_4)$$\n",
    "\n",
    "Note also taht: \n",
    "$$ \\frac{\\partial \\ell}{\\partial w_i} = -2(p_i-w_i) = -2(p_i-p_i^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([2., 2., 2., 2.], requires_grad=True)\n",
      "w= 2*p: tensor([4., 4., 4., 4.], grad_fn=<PowBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = torch.tensor([2.0, 2.0,2.0, 2.0], requires_grad=True)\n",
    "w = p**2\n",
    "\n",
    "# we define p and w just like before, in a tensor fashion now.\n",
    "print('p:', p, end='\\n')\n",
    "print('w= 2*p:', w, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to call `w.backward()`, we get an error since w is not a scalar and backward has to be called on scalar tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/scvi-dev/scvi-tools/scvi-env/lib/python3.12/site-packages/torch/autograd/__init__.py:198\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "w.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(p: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    tmp = (p-w)**2\n",
    "    return tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(16., grad_fn=<SumBackward0>)\n",
      "\n",
      "grad of p before calling l.backward() --> p.grad: None\n",
      "\n",
      "grad of p after calling l.backward() on loss --> p.grad: tensor([12., 12., 12., 12.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = torch.tensor([2.0, 2.0,2.0, 2.0], requires_grad=True)\n",
    "w = p**2\n",
    "l = loss_fn(p,w)\n",
    "# now we can call backward on l, meaning we are computing dl/dp\n",
    "print('loss:', l, end='\\n\\n')\n",
    "\n",
    "print('grad of p before calling l.backward() --> p.grad:', p.grad, end='\\n\\n')\n",
    "\n",
    "# backward\n",
    "l.backward()\n",
    "print('grad of p after calling l.backward() on loss --> p.grad:', p.grad, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if p_i=2, $4*2^3 - 6*2^2 +2*2 = 32 - 24 + 4 = 12$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, grad of w after calling l.backward() is no longer computable straightforardly, since w is not leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/r059lv3n7nq9x6gh78v25xk80000gn/T/ipykernel_62126/2961328297.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what about dl/dw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 12., 12., 12.])\n",
      "tensor([4., 4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "#Note: what about dl/dw?\n",
    "p = torch.tensor([2.0, 2.0,2.0, 2.0], requires_grad=True)\n",
    "w = p**2\n",
    "w.retain_grad()\n",
    "l = loss_fn(p,w)\n",
    "l.backward()\n",
    "print(p.grad)\n",
    "print(w.grad) # -2 (p_i - p_i^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far, \n",
    "1. take some tensor p\n",
    "2. do operations on it\n",
    "3. compute a scalar number (the loss)\n",
    "4. compute dl/dp.\n",
    "\n",
    "\n",
    "Now it's time to change p according to dl/dp!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer\n",
    "\n",
    "To do so, in pytorch we use the optimizer. In particular, we'll see two fundamental methods.\n",
    "\n",
    "`.zero_grad()` --> set to zero the `.grad` attributeof all leaf tensors \\\n",
    "`.step()` --> updates the leaf `p` by `p_new = p - lr * p.grad` where `lr` is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's go back to the previous example, changing a little bit numbers.\n",
    "\n",
    "Let's define the following: $\\\\ p = (1,3,2,2) \\\\ w = p^2$ \n",
    "$$ \\ell(p,w) = \\sum_{i=1}^{4} (p_i - w_i)^2 = \\sum_{i=1}^{4} (p_i - p_i^2)^2 = \\ell(p). $$\n",
    "We already know that $$ \\frac{\\partial \\ell}{\\partial p_i} = 2(p_i-p_i^2)(1-2p_i) = 4p_i^3-6p_i^2+2p_i $$\n",
    "and we know that the content of `p.grad` is simply:\n",
    "$$ \\nabla_p \\ell = (\\frac{\\partial \\ell}{\\partial p_1}, \\frac{\\partial \\ell}{\\partial p_2}, \\frac{\\partial \\ell}{\\partial p_3}, \\frac{\\partial \\ell}{\\partial p_4}) = (4p_1^3-6p_1^2+2p_1, 4p_2^3-6p_2^2+2p_2, 4p_3^3-6p_3^2+2p_3, 4p_4^3-6p_4^2+2p_4)$$\n",
    "which in our case is:\n",
    "$$p.grad = \\nabla_p \\ell = (4-6+2, 108-54+6, 32-24+4, 32-24+4) = (0, 60, 12, 12)$$\n",
    "\n",
    "When we call `optimizer.step()` with learning rate $\\eta$ at time $t$, we get:\n",
    "$$p_{i, next} = p_{i,before} - \\eta * \\frac{\\partial \\ell}{\\partial p_i} = p_{i,before} - \\eta * (4p_i^3-6p_i^2+2p_i)$$\n",
    "\n",
    "Putting numbers together, we get:\n",
    "$$p_{1, next} = 1 - \\eta * 0 = 1, \\\\\n",
    "p_{2, next} = 3 - \\eta * 60 = 3 - 60\\eta, \\\\\n",
    "p_{3, next} = 2 - \\eta * 12 = 2 - 12\\eta, \\\\\n",
    "p_{4, next} = 2 - \\eta * 12 = 2 - 12\\eta.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: tensor([1., 3., 2., 2.], requires_grad=True)\n",
      "w= 2*p: tensor([1., 9., 4., 4.], grad_fn=<PowBackward0>)\n",
      "loss: tensor(44., grad_fn=<SumBackward0>)\n",
      "grad of p before calling backward() --> p.grad: None\n",
      "define lr=1\n",
      "grad of p after calling backward() on loss --> p.grad: tensor([ 0., 60., 12., 12.])\n",
      "p after calling opt.step() --> p: tensor([  1., -57., -10., -10.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = torch.tensor([1.0, 3.0, 2.0, 2.0], requires_grad=True)\n",
    "w = p**2\n",
    "\n",
    "print('v:', p)\n",
    "print('w= 2*p:', w)\n",
    "\n",
    "l = loss_fn(p,w)\n",
    "\n",
    "print('loss:', l)\n",
    "\n",
    "print('grad of p before calling backward() --> p.grad:', p.grad)\n",
    "print('define lr=1')\n",
    "opt = torch.optim.SGD([p], lr=1)\n",
    "opt.zero_grad()\n",
    "# backward\n",
    "l.backward()\n",
    "print('grad of p after calling backward() on loss --> p.grad:', p.grad)\n",
    "\n",
    "\n",
    "opt.step()\n",
    "print('p after calling opt.step() --> p:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function that takes an input and it also depends on some parameters.\n",
    "Then, it combines the input with parameters and returns an output.\n",
    "\n",
    "Note that this is exactley what we do with NN: we have some parameters (the weights and biases) and we have some input (the data) and we get an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([ 1.5410, -0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "def a_function(input_tensor: torch.Tensor, p: torch.Tensor) -> torch.Tensor:\n",
    "    return input_tensor*p\n",
    "\n",
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "print('p:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a dataset of 100 random points in 2d.\n",
    "This will be our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1258, -1.1524])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dataset = torch.randn([100, 2])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRUCIAL point** of deep learning:\n",
    "\n",
    "we will define the task we want to solve using our \"function\", and based on this we define a proper loss function to achieve our task.\n",
    "\n",
    "The task is simple: the function has to learn how to multiply numbers by 2.\n",
    "\n",
    "So the loss will be defined according to this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input: torch.Tensor, output: torch.Tensor):\n",
    "    \"\"\"\n",
    "    input: will be a datapoint / a batch of datapoints\n",
    "    output: will be the output of the function, ie output = a_function(input, p)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(input: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "    square_diff = (output - 2*input)**2\n",
    "    return square_diff.mean(-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we learn the task perfectly, then the loss will be 0.\n",
    "\n",
    "Let's use what we know about SGD optimizer to solve this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final parameters: tensor([1.8175, 1.1722], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "\n",
    "#TODO:\n",
    "# 1. play with the lr and the dimension of the dataset (here it is 100)\n",
    "# 2. print things so that you familiarize yourself with the code\n",
    "\n",
    "for data in dataset:\n",
    "    # one update per each datapoint!!!\n",
    "    #print(data)\n",
    "    #break\n",
    "    output = a_function(input_tensor=data, p=p)\n",
    "    #print(output.size())\n",
    "    #break\n",
    "    l = loss(data, output)\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "print('final parameters:', p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the calculations using all dataset at once! Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters: tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "output: torch.Size([100, 2])\n",
      "torch.Size([])\n",
      "final parameters: tensor([ 1.5452, -0.2704], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "print('initial parameters:', p)\n",
    "\n",
    "output = a_function(input_tensor=dataset, p=p)\n",
    "print('output:', output.size())\n",
    "l = loss(dataset, output)\n",
    "print(l.size())\n",
    "opt.zero_grad()\n",
    "l.backward() # nonly one update!!!!\n",
    "opt.step()\n",
    "\n",
    "print('final parameters:', p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can either do one update (`l.backward()` + `opt.step()`) for each datapoint (100 updates), or can take all the dataset at once and do one update (1 update)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we pass through **all** the dataset, we call it an **epoch**.\n",
    "\n",
    "Now we see both cases with epochs.\n",
    "\n",
    "Note!!! In the first case (one update per datapoint), in the end we will do 100 x num_epochs updates.\n",
    "\n",
    "In the second case (one update per epoch), in the end we will do num_epochs updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters: tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 0:  tensor([1.8175, 1.1722], requires_grad=True)\n",
      "epoch 1:  tensor([1.9274, 1.7012], requires_grad=True)\n",
      "epoch 2:  tensor([1.9711, 1.8922], requires_grad=True)\n",
      "epoch 3:  tensor([1.9885, 1.9611], requires_grad=True)\n",
      "epoch 4:  tensor([1.9954, 1.9860], requires_grad=True)\n",
      "epoch 5:  tensor([1.9982, 1.9949], requires_grad=True)\n",
      "epoch 6:  tensor([1.9993, 1.9982], requires_grad=True)\n",
      "epoch 7:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 8:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 9:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "final parameters: tensor([2.0000, 1.9999], requires_grad=True)\n",
      "n_updates: 1000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "n_updates = 0\n",
    "print('initial parameters:', p)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for data in dataset:\n",
    "        # always updating p for each datapoint\n",
    "        output = a_function(input_tensor=data, p=p)\n",
    "        l = loss(data, output)\n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        n_updates += 1\n",
    "\n",
    "    print(f'epoch {epoch}: ', p)\n",
    "\n",
    "print('final parameters:', p)\n",
    "print('n_updates:', n_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters: tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 0:  tensor([ 1.5828, -0.0630], requires_grad=True)\n",
      "epoch 1:  tensor([1.6209, 0.1442], requires_grad=True)\n",
      "epoch 2:  tensor([1.6554, 0.3307], requires_grad=True)\n",
      "epoch 3:  tensor([1.6868, 0.4984], requires_grad=True)\n",
      "epoch 4:  tensor([1.7154, 0.6492], requires_grad=True)\n",
      "epoch 5:  tensor([1.7413, 0.7849], requires_grad=True)\n",
      "epoch 6:  tensor([1.7649, 0.9070], requires_grad=True)\n",
      "epoch 7:  tensor([1.7863, 1.0168], requires_grad=True)\n",
      "epoch 8:  tensor([1.8058, 1.1156], requires_grad=True)\n",
      "epoch 9:  tensor([1.8235, 1.2044], requires_grad=True)\n",
      "epoch 10:  tensor([1.8396, 1.2843], requires_grad=True)\n",
      "epoch 11:  tensor([1.8542, 1.3562], requires_grad=True)\n",
      "epoch 12:  tensor([1.8675, 1.4209], requires_grad=True)\n",
      "epoch 13:  tensor([1.8796, 1.4791], requires_grad=True)\n",
      "epoch 14:  tensor([1.8905, 1.5314], requires_grad=True)\n",
      "epoch 15:  tensor([1.9005, 1.5785], requires_grad=True)\n",
      "epoch 16:  tensor([1.9096, 1.6208], requires_grad=True)\n",
      "epoch 17:  tensor([1.9178, 1.6589], requires_grad=True)\n",
      "epoch 18:  tensor([1.9253, 1.6932], requires_grad=True)\n",
      "epoch 19:  tensor([1.9321, 1.7240], requires_grad=True)\n",
      "epoch 20:  tensor([1.9383, 1.7517], requires_grad=True)\n",
      "epoch 21:  tensor([1.9439, 1.7767], requires_grad=True)\n",
      "epoch 22:  tensor([1.9490, 1.7991], requires_grad=True)\n",
      "epoch 23:  tensor([1.9537, 1.8193], requires_grad=True)\n",
      "epoch 24:  tensor([1.9579, 1.8375], requires_grad=True)\n",
      "epoch 25:  tensor([1.9617, 1.8538], requires_grad=True)\n",
      "epoch 26:  tensor([1.9652, 1.8685], requires_grad=True)\n",
      "epoch 27:  tensor([1.9684, 1.8817], requires_grad=True)\n",
      "epoch 28:  tensor([1.9713, 1.8936], requires_grad=True)\n",
      "epoch 29:  tensor([1.9739, 1.9043], requires_grad=True)\n",
      "epoch 30:  tensor([1.9763, 1.9139], requires_grad=True)\n",
      "epoch 31:  tensor([1.9784, 1.9225], requires_grad=True)\n",
      "epoch 32:  tensor([1.9804, 1.9303], requires_grad=True)\n",
      "epoch 33:  tensor([1.9822, 1.9373], requires_grad=True)\n",
      "epoch 34:  tensor([1.9838, 1.9436], requires_grad=True)\n",
      "epoch 35:  tensor([1.9853, 1.9493], requires_grad=True)\n",
      "epoch 36:  tensor([1.9866, 1.9544], requires_grad=True)\n",
      "epoch 37:  tensor([1.9879, 1.9590], requires_grad=True)\n",
      "epoch 38:  tensor([1.9890, 1.9631], requires_grad=True)\n",
      "epoch 39:  tensor([1.9900, 1.9668], requires_grad=True)\n",
      "epoch 40:  tensor([1.9909, 1.9701], requires_grad=True)\n",
      "epoch 41:  tensor([1.9917, 1.9731], requires_grad=True)\n",
      "epoch 42:  tensor([1.9925, 1.9758], requires_grad=True)\n",
      "epoch 43:  tensor([1.9932, 1.9783], requires_grad=True)\n",
      "epoch 44:  tensor([1.9938, 1.9804], requires_grad=True)\n",
      "epoch 45:  tensor([1.9943, 1.9824], requires_grad=True)\n",
      "epoch 46:  tensor([1.9949, 1.9842], requires_grad=True)\n",
      "epoch 47:  tensor([1.9953, 1.9858], requires_grad=True)\n",
      "epoch 48:  tensor([1.9958, 1.9872], requires_grad=True)\n",
      "epoch 49:  tensor([1.9961, 1.9885], requires_grad=True)\n",
      "epoch 50:  tensor([1.9965, 1.9896], requires_grad=True)\n",
      "epoch 51:  tensor([1.9968, 1.9907], requires_grad=True)\n",
      "epoch 52:  tensor([1.9971, 1.9916], requires_grad=True)\n",
      "epoch 53:  tensor([1.9974, 1.9925], requires_grad=True)\n",
      "epoch 54:  tensor([1.9976, 1.9932], requires_grad=True)\n",
      "epoch 55:  tensor([1.9978, 1.9939], requires_grad=True)\n",
      "epoch 56:  tensor([1.9980, 1.9945], requires_grad=True)\n",
      "epoch 57:  tensor([1.9982, 1.9951], requires_grad=True)\n",
      "epoch 58:  tensor([1.9984, 1.9956], requires_grad=True)\n",
      "epoch 59:  tensor([1.9985, 1.9960], requires_grad=True)\n",
      "epoch 60:  tensor([1.9987, 1.9964], requires_grad=True)\n",
      "epoch 61:  tensor([1.9988, 1.9968], requires_grad=True)\n",
      "epoch 62:  tensor([1.9989, 1.9971], requires_grad=True)\n",
      "epoch 63:  tensor([1.9990, 1.9974], requires_grad=True)\n",
      "epoch 64:  tensor([1.9991, 1.9976], requires_grad=True)\n",
      "epoch 65:  tensor([1.9992, 1.9979], requires_grad=True)\n",
      "epoch 66:  tensor([1.9992, 1.9981], requires_grad=True)\n",
      "epoch 67:  tensor([1.9993, 1.9983], requires_grad=True)\n",
      "epoch 68:  tensor([1.9994, 1.9985], requires_grad=True)\n",
      "epoch 69:  tensor([1.9994, 1.9986], requires_grad=True)\n",
      "epoch 70:  tensor([1.9995, 1.9988], requires_grad=True)\n",
      "epoch 71:  tensor([1.9995, 1.9989], requires_grad=True)\n",
      "epoch 72:  tensor([1.9996, 1.9990], requires_grad=True)\n",
      "epoch 73:  tensor([1.9996, 1.9991], requires_grad=True)\n",
      "epoch 74:  tensor([1.9996, 1.9992], requires_grad=True)\n",
      "epoch 75:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 76:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 77:  tensor([1.9997, 1.9994], requires_grad=True)\n",
      "epoch 78:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 79:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 80:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 81:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 82:  tensor([1.9998, 1.9997], requires_grad=True)\n",
      "epoch 83:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 84:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 85:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 86:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 87:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 88:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 89:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 90:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 91:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 92:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 93:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 94:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 95:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "epoch 96:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "epoch 97:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "epoch 98:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "epoch 99:  tensor([2.0000, 1.9999], requires_grad=True)\n",
      "final parameters: tensor([2.0000, 1.9999], requires_grad=True)\n",
      "n_updates: 100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.1)\n",
    "print('initial parameters:', p)\n",
    "n_updates = 0\n",
    "for epoch in range(100):\n",
    "    # one update only!!!\n",
    "    output = a_function(input_tensor=dataset, p=p)\n",
    "    l = loss(dataset, output)\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    n_updates += 1\n",
    "    print(f'epoch {epoch}: ', p)\n",
    "\n",
    "print('final parameters:', p)\n",
    "print('n_updates:', n_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almost mini-batches :)\n",
    "So we see that we can do one update per each datapoint, or one update using all the dataset at once: then, we can iterate this process for some epochs.\n",
    "\n",
    "Actually, there is an intermediate way: we can use a batch of datapoints to do one update. This is the most common way to do it in practice.\n",
    "\n",
    "We'll see this in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digression on broadcasting in Pytorch\n",
    "\n",
    "Sorry for being messy during this part today ðŸ˜…\n",
    "\n",
    "I'll try to explain again why both the `a_function` and `loss` functions work with both single points and the whole dataset.\n",
    "\n",
    "```\n",
    "def a_function(input_tensor, p):\n",
    "    return input_tensor*p\n",
    "\n",
    "def loss(input, output):\n",
    "    return (output - 2*input).pow(2).mean(-1).mean()\n",
    "```\n",
    "\n",
    "1. `p` will **always** (in this example) be a tensor of shape `(2,)` (es $[1.0, 1.0]$).\n",
    "\n",
    "2. If we process a single point, `input_tensor` will be a tensor of shape `(2,)` (es $[0.5, 2.0]$). \\\n",
    "and `output` will be a tensor of shape `(2,)` (es $[1.0*0.5, 1.0*2.0] = [0.5, 2.0]$).\\\n",
    "When we compute the loss, we get that: `output - 2*input` will be a tensor of shape `(2,)` (es $[0.5 - 2*0.5, 2.0 - 2*2.0] = [0.5 - 1.0, 2.0 - 4.0] = [-0.5, -2.0]$).\\\n",
    "Then, the  `.pow(2)` is not a problem, we simply get $[0.25, 4.0]$.\\\n",
    "Then, the `.mean(-1)` will take the mean of the last dimension, which **IN THIS CASE** is the only dimension, so we get $[2.125]$.\\\n",
    "Then, the `.mean()` will take the mean of the only element, so we get 2.125. \\\n",
    "**!!!! In this case the last `.mean()` is not necessary, but it is there to make the function work with both single points and the whole dataset.**\n",
    "\n",
    "3. Now, if we process the whole dataset, `input_tensor` will have a shape `(N, 2)`, where N is the number of data points.\\\n",
    "Since `p` is a tensor of shape `(2,)`, the broadcasting will make the multiplication element-wise: so it's like multiplying each row of `input_tensor` (which is now the entire dataset!) by `p`.\\\n",
    "Hence the output will be a tensor of shape `(N, 2)` (where each original point is multiplied element-wise by `p`).\\\n",
    "So in this case, `input=input_tensor=dataset` and `output` are tensors of shape `(N, 2)`.\\\n",
    "The subtraction `output - 2*input` is also broadcasted element-wise, preserving the shape `(N, 2)`.\\\n",
    "The `.pow(2)` operation is also applied element-wise, still keeping the shape `(N, 2)`.\\\n",
    "The `.mean(-1)` computes the mean along the last dimension (dim=-1), reducing the shape to (N,) (i.e., **we now have a single loss value for each data point! So we are computing the right loss for each datapoint, all at once!!! Think about it, this is the logics behind batch-based training.**).\n",
    "The final `.mean()` takes the mean over all N points, producing a single scalar loss value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small analyis on accumulating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4184, -2.3040])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "\n",
    "output = a_function(input_tensor=dataset, p=p)\n",
    "l = loss(dataset, output)\n",
    "opt.zero_grad()\n",
    "l.backward()\n",
    "print(p.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -41.8369, -230.4015])\n",
      "tensor([-0.4184, -2.3040])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "\n",
    "opt.zero_grad()\n",
    "for data in dataset:\n",
    "    output = a_function(input_tensor=data, p=p)\n",
    "    l = loss(data, output)\n",
    "    l.backward() # this populate the grad attribute of p, accumulating the gradients\n",
    "\n",
    "print(p.grad)\n",
    "print(p.grad/len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4184, -2.3040])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss_total = 0\n",
    "for data in dataset:\n",
    "    output = a_function(input_tensor=data, p=p)\n",
    "    l = loss(data, output)\n",
    "    loss_total += l\n",
    "\n",
    "loss_total = loss_total/len(dataset)\n",
    "loss_total.backward()\n",
    "print(p.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -41.8369, -230.4015])\n",
      "tensor([-0.4184, -2.3040])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "dataset = torch.randn([100, 2])\n",
    "opt = torch.optim.SGD([p], lr=0.01)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss_total = 0\n",
    "for data in dataset:\n",
    "    output = a_function(input_tensor=data, p=p)\n",
    "    l = loss(data, output)\n",
    "    loss_total += l\n",
    "\n",
    "loss_total.backward()\n",
    "print(p.grad)\n",
    "print(p.grad/len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: write you the SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  tensor([ 1.5810, -0.0739], requires_grad=True)\n",
      "epoch 1:  tensor([1.6176, 0.1246], requires_grad=True)\n",
      "epoch 2:  tensor([1.6509, 0.3041], requires_grad=True)\n",
      "epoch 3:  tensor([1.6814, 0.4664], requires_grad=True)\n",
      "epoch 4:  tensor([1.7092, 0.6131], requires_grad=True)\n",
      "epoch 5:  tensor([1.7345, 0.7459], requires_grad=True)\n",
      "epoch 6:  tensor([1.7577, 0.8659], requires_grad=True)\n",
      "epoch 7:  tensor([1.7788, 0.9745], requires_grad=True)\n",
      "epoch 8:  tensor([1.7981, 1.0726], requires_grad=True)\n",
      "epoch 9:  tensor([1.8157, 1.1614], requires_grad=True)\n",
      "epoch 10:  tensor([1.8318, 1.2416], requires_grad=True)\n",
      "epoch 11:  tensor([1.8464, 1.3142], requires_grad=True)\n",
      "epoch 12:  tensor([1.8598, 1.3798], requires_grad=True)\n",
      "epoch 13:  tensor([1.8721, 1.4392], requires_grad=True)\n",
      "epoch 14:  tensor([1.8832, 1.4929], requires_grad=True)\n",
      "epoch 15:  tensor([1.8934, 1.5414], requires_grad=True)\n",
      "epoch 16:  tensor([1.9027, 1.5853], requires_grad=True)\n",
      "epoch 17:  tensor([1.9112, 1.6250], requires_grad=True)\n",
      "epoch 18:  tensor([1.9189, 1.6609], requires_grad=True)\n",
      "epoch 19:  tensor([1.9260, 1.6933], requires_grad=True)\n",
      "epoch 20:  tensor([1.9325, 1.7227], requires_grad=True)\n",
      "epoch 21:  tensor([1.9383, 1.7492], requires_grad=True)\n",
      "epoch 22:  tensor([1.9437, 1.7732], requires_grad=True)\n",
      "epoch 23:  tensor([1.9486, 1.7949], requires_grad=True)\n",
      "epoch 24:  tensor([1.9531, 1.8146], requires_grad=True)\n",
      "epoch 25:  tensor([1.9572, 1.8323], requires_grad=True)\n",
      "epoch 26:  tensor([1.9609, 1.8484], requires_grad=True)\n",
      "epoch 27:  tensor([1.9643, 1.8629], requires_grad=True)\n",
      "epoch 28:  tensor([1.9675, 1.8760], requires_grad=True)\n",
      "epoch 29:  tensor([1.9703, 1.8879], requires_grad=True)\n",
      "epoch 30:  tensor([1.9729, 1.8986], requires_grad=True)\n",
      "epoch 31:  tensor([1.9752, 1.9083], requires_grad=True)\n",
      "epoch 32:  tensor([1.9774, 1.9171], requires_grad=True)\n",
      "epoch 33:  tensor([1.9794, 1.9250], requires_grad=True)\n",
      "epoch 34:  tensor([1.9812, 1.9322], requires_grad=True)\n",
      "epoch 35:  tensor([1.9828, 1.9387], requires_grad=True)\n",
      "epoch 36:  tensor([1.9843, 1.9445], requires_grad=True)\n",
      "epoch 37:  tensor([1.9857, 1.9499], requires_grad=True)\n",
      "epoch 38:  tensor([1.9869, 1.9547], requires_grad=True)\n",
      "epoch 39:  tensor([1.9881, 1.9590], requires_grad=True)\n",
      "epoch 40:  tensor([1.9891, 1.9629], requires_grad=True)\n",
      "epoch 41:  tensor([1.9901, 1.9665], requires_grad=True)\n",
      "epoch 42:  tensor([1.9909, 1.9697], requires_grad=True)\n",
      "epoch 43:  tensor([1.9917, 1.9726], requires_grad=True)\n",
      "epoch 44:  tensor([1.9924, 1.9752], requires_grad=True)\n",
      "epoch 45:  tensor([1.9931, 1.9776], requires_grad=True)\n",
      "epoch 46:  tensor([1.9937, 1.9797], requires_grad=True)\n",
      "epoch 47:  tensor([1.9943, 1.9817], requires_grad=True)\n",
      "epoch 48:  tensor([1.9948, 1.9834], requires_grad=True)\n",
      "epoch 49:  tensor([1.9952, 1.9850], requires_grad=True)\n",
      "epoch 50:  tensor([1.9956, 1.9864], requires_grad=True)\n",
      "epoch 51:  tensor([1.9960, 1.9877], requires_grad=True)\n",
      "epoch 52:  tensor([1.9964, 1.9889], requires_grad=True)\n",
      "epoch 53:  tensor([1.9967, 1.9900], requires_grad=True)\n",
      "epoch 54:  tensor([1.9970, 1.9909], requires_grad=True)\n",
      "epoch 55:  tensor([1.9972, 1.9918], requires_grad=True)\n",
      "epoch 56:  tensor([1.9975, 1.9926], requires_grad=True)\n",
      "epoch 57:  tensor([1.9977, 1.9933], requires_grad=True)\n",
      "epoch 58:  tensor([1.9979, 1.9939], requires_grad=True)\n",
      "epoch 59:  tensor([1.9981, 1.9945], requires_grad=True)\n",
      "epoch 60:  tensor([1.9982, 1.9950], requires_grad=True)\n",
      "epoch 61:  tensor([1.9984, 1.9955], requires_grad=True)\n",
      "epoch 62:  tensor([1.9985, 1.9959], requires_grad=True)\n",
      "epoch 63:  tensor([1.9987, 1.9963], requires_grad=True)\n",
      "epoch 64:  tensor([1.9988, 1.9967], requires_grad=True)\n",
      "epoch 65:  tensor([1.9989, 1.9970], requires_grad=True)\n",
      "epoch 66:  tensor([1.9990, 1.9973], requires_grad=True)\n",
      "epoch 67:  tensor([1.9991, 1.9975], requires_grad=True)\n",
      "epoch 68:  tensor([1.9992, 1.9978], requires_grad=True)\n",
      "epoch 69:  tensor([1.9992, 1.9980], requires_grad=True)\n",
      "epoch 70:  tensor([1.9993, 1.9982], requires_grad=True)\n",
      "epoch 71:  tensor([1.9994, 1.9984], requires_grad=True)\n",
      "epoch 72:  tensor([1.9994, 1.9985], requires_grad=True)\n",
      "epoch 73:  tensor([1.9995, 1.9987], requires_grad=True)\n",
      "epoch 74:  tensor([1.9995, 1.9988], requires_grad=True)\n",
      "epoch 75:  tensor([1.9996, 1.9989], requires_grad=True)\n",
      "epoch 76:  tensor([1.9996, 1.9990], requires_grad=True)\n",
      "epoch 77:  tensor([1.9996, 1.9991], requires_grad=True)\n",
      "epoch 78:  tensor([1.9997, 1.9992], requires_grad=True)\n",
      "epoch 79:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 80:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 81:  tensor([1.9997, 1.9994], requires_grad=True)\n",
      "epoch 82:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 83:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 84:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 85:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 86:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 87:  tensor([1.9998, 1.9997], requires_grad=True)\n",
      "epoch 88:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 89:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 90:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 91:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 92:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 93:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 94:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 95:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 96:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 97:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 98:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 99:  tensor([1.9999, 1.9999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "lr = 0.001\n",
    "\n",
    "for epoch in range(100):\n",
    "    for data in dataset:\n",
    "        output = a_function(input_tensor=data, p=p)\n",
    "        l = loss(data, output)\n",
    "\n",
    "        p.grad = torch.zeros_like(p)  # reset gradients\n",
    "        l.backward()\n",
    "        \n",
    "        with torch.no_grad():  # prevents torch from tracking the update\n",
    "            p -= lr * p.grad  \n",
    "\n",
    "    print(f'epoch {epoch}: ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  tensor([ 1.5810, -0.0739], requires_grad=True)\n",
      "epoch 1:  tensor([1.6176, 0.1246], requires_grad=True)\n",
      "epoch 2:  tensor([1.6509, 0.3041], requires_grad=True)\n",
      "epoch 3:  tensor([1.6814, 0.4664], requires_grad=True)\n",
      "epoch 4:  tensor([1.7092, 0.6131], requires_grad=True)\n",
      "epoch 5:  tensor([1.7345, 0.7459], requires_grad=True)\n",
      "epoch 6:  tensor([1.7577, 0.8659], requires_grad=True)\n",
      "epoch 7:  tensor([1.7788, 0.9745], requires_grad=True)\n",
      "epoch 8:  tensor([1.7981, 1.0726], requires_grad=True)\n",
      "epoch 9:  tensor([1.8157, 1.1614], requires_grad=True)\n",
      "epoch 10:  tensor([1.8318, 1.2416], requires_grad=True)\n",
      "epoch 11:  tensor([1.8464, 1.3142], requires_grad=True)\n",
      "epoch 12:  tensor([1.8598, 1.3798], requires_grad=True)\n",
      "epoch 13:  tensor([1.8721, 1.4392], requires_grad=True)\n",
      "epoch 14:  tensor([1.8832, 1.4929], requires_grad=True)\n",
      "epoch 15:  tensor([1.8934, 1.5414], requires_grad=True)\n",
      "epoch 16:  tensor([1.9027, 1.5853], requires_grad=True)\n",
      "epoch 17:  tensor([1.9112, 1.6250], requires_grad=True)\n",
      "epoch 18:  tensor([1.9189, 1.6609], requires_grad=True)\n",
      "epoch 19:  tensor([1.9260, 1.6933], requires_grad=True)\n",
      "epoch 20:  tensor([1.9325, 1.7227], requires_grad=True)\n",
      "epoch 21:  tensor([1.9383, 1.7492], requires_grad=True)\n",
      "epoch 22:  tensor([1.9437, 1.7732], requires_grad=True)\n",
      "epoch 23:  tensor([1.9486, 1.7949], requires_grad=True)\n",
      "epoch 24:  tensor([1.9531, 1.8146], requires_grad=True)\n",
      "epoch 25:  tensor([1.9572, 1.8323], requires_grad=True)\n",
      "epoch 26:  tensor([1.9609, 1.8484], requires_grad=True)\n",
      "epoch 27:  tensor([1.9643, 1.8629], requires_grad=True)\n",
      "epoch 28:  tensor([1.9675, 1.8760], requires_grad=True)\n",
      "epoch 29:  tensor([1.9703, 1.8879], requires_grad=True)\n",
      "epoch 30:  tensor([1.9729, 1.8986], requires_grad=True)\n",
      "epoch 31:  tensor([1.9752, 1.9083], requires_grad=True)\n",
      "epoch 32:  tensor([1.9774, 1.9171], requires_grad=True)\n",
      "epoch 33:  tensor([1.9794, 1.9250], requires_grad=True)\n",
      "epoch 34:  tensor([1.9812, 1.9322], requires_grad=True)\n",
      "epoch 35:  tensor([1.9828, 1.9387], requires_grad=True)\n",
      "epoch 36:  tensor([1.9843, 1.9445], requires_grad=True)\n",
      "epoch 37:  tensor([1.9857, 1.9499], requires_grad=True)\n",
      "epoch 38:  tensor([1.9869, 1.9547], requires_grad=True)\n",
      "epoch 39:  tensor([1.9881, 1.9590], requires_grad=True)\n",
      "epoch 40:  tensor([1.9891, 1.9629], requires_grad=True)\n",
      "epoch 41:  tensor([1.9901, 1.9665], requires_grad=True)\n",
      "epoch 42:  tensor([1.9909, 1.9697], requires_grad=True)\n",
      "epoch 43:  tensor([1.9917, 1.9726], requires_grad=True)\n",
      "epoch 44:  tensor([1.9924, 1.9752], requires_grad=True)\n",
      "epoch 45:  tensor([1.9931, 1.9776], requires_grad=True)\n",
      "epoch 46:  tensor([1.9937, 1.9797], requires_grad=True)\n",
      "epoch 47:  tensor([1.9943, 1.9817], requires_grad=True)\n",
      "epoch 48:  tensor([1.9948, 1.9834], requires_grad=True)\n",
      "epoch 49:  tensor([1.9952, 1.9850], requires_grad=True)\n",
      "epoch 50:  tensor([1.9956, 1.9864], requires_grad=True)\n",
      "epoch 51:  tensor([1.9960, 1.9877], requires_grad=True)\n",
      "epoch 52:  tensor([1.9964, 1.9889], requires_grad=True)\n",
      "epoch 53:  tensor([1.9967, 1.9900], requires_grad=True)\n",
      "epoch 54:  tensor([1.9970, 1.9909], requires_grad=True)\n",
      "epoch 55:  tensor([1.9972, 1.9918], requires_grad=True)\n",
      "epoch 56:  tensor([1.9975, 1.9926], requires_grad=True)\n",
      "epoch 57:  tensor([1.9977, 1.9933], requires_grad=True)\n",
      "epoch 58:  tensor([1.9979, 1.9939], requires_grad=True)\n",
      "epoch 59:  tensor([1.9981, 1.9945], requires_grad=True)\n",
      "epoch 60:  tensor([1.9982, 1.9950], requires_grad=True)\n",
      "epoch 61:  tensor([1.9984, 1.9955], requires_grad=True)\n",
      "epoch 62:  tensor([1.9985, 1.9959], requires_grad=True)\n",
      "epoch 63:  tensor([1.9987, 1.9963], requires_grad=True)\n",
      "epoch 64:  tensor([1.9988, 1.9967], requires_grad=True)\n",
      "epoch 65:  tensor([1.9989, 1.9970], requires_grad=True)\n",
      "epoch 66:  tensor([1.9990, 1.9973], requires_grad=True)\n",
      "epoch 67:  tensor([1.9991, 1.9975], requires_grad=True)\n",
      "epoch 68:  tensor([1.9992, 1.9978], requires_grad=True)\n",
      "epoch 69:  tensor([1.9992, 1.9980], requires_grad=True)\n",
      "epoch 70:  tensor([1.9993, 1.9982], requires_grad=True)\n",
      "epoch 71:  tensor([1.9994, 1.9984], requires_grad=True)\n",
      "epoch 72:  tensor([1.9994, 1.9985], requires_grad=True)\n",
      "epoch 73:  tensor([1.9995, 1.9987], requires_grad=True)\n",
      "epoch 74:  tensor([1.9995, 1.9988], requires_grad=True)\n",
      "epoch 75:  tensor([1.9996, 1.9989], requires_grad=True)\n",
      "epoch 76:  tensor([1.9996, 1.9990], requires_grad=True)\n",
      "epoch 77:  tensor([1.9996, 1.9991], requires_grad=True)\n",
      "epoch 78:  tensor([1.9997, 1.9992], requires_grad=True)\n",
      "epoch 79:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 80:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 81:  tensor([1.9997, 1.9994], requires_grad=True)\n",
      "epoch 82:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 83:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 84:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 85:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 86:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 87:  tensor([1.9998, 1.9997], requires_grad=True)\n",
      "epoch 88:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 89:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 90:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 91:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 92:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 93:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 94:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 95:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 96:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 97:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 98:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 99:  tensor([1.9999, 1.9999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# this is the solution given by ChatGPT when I asked to solve the problem... a little bit too messy\n",
    "torch.manual_seed(0)\n",
    "dim = 2  \n",
    "lr = 0.001  \n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for data in dataset:\n",
    "        output = a_function(input_tensor=data, p=p)\n",
    "        l = loss(data, output)\n",
    "        l.backward()\n",
    "\n",
    "        p_new = (p - lr * p.grad).clone().detach().requires_grad_(True)\n",
    "        p.grad.zero_()  \n",
    "        p = p_new  \n",
    "\n",
    "    print(f'epoch {epoch}: ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  tensor([ 1.5810, -0.0739], requires_grad=True)\n",
      "epoch 1:  tensor([1.6176, 0.1246], requires_grad=True)\n",
      "epoch 2:  tensor([1.6509, 0.3041], requires_grad=True)\n",
      "epoch 3:  tensor([1.6814, 0.4664], requires_grad=True)\n",
      "epoch 4:  tensor([1.7092, 0.6131], requires_grad=True)\n",
      "epoch 5:  tensor([1.7345, 0.7459], requires_grad=True)\n",
      "epoch 6:  tensor([1.7577, 0.8659], requires_grad=True)\n",
      "epoch 7:  tensor([1.7788, 0.9745], requires_grad=True)\n",
      "epoch 8:  tensor([1.7981, 1.0726], requires_grad=True)\n",
      "epoch 9:  tensor([1.8157, 1.1614], requires_grad=True)\n",
      "epoch 10:  tensor([1.8318, 1.2416], requires_grad=True)\n",
      "epoch 11:  tensor([1.8464, 1.3142], requires_grad=True)\n",
      "epoch 12:  tensor([1.8598, 1.3798], requires_grad=True)\n",
      "epoch 13:  tensor([1.8721, 1.4392], requires_grad=True)\n",
      "epoch 14:  tensor([1.8832, 1.4929], requires_grad=True)\n",
      "epoch 15:  tensor([1.8934, 1.5414], requires_grad=True)\n",
      "epoch 16:  tensor([1.9027, 1.5853], requires_grad=True)\n",
      "epoch 17:  tensor([1.9112, 1.6250], requires_grad=True)\n",
      "epoch 18:  tensor([1.9189, 1.6609], requires_grad=True)\n",
      "epoch 19:  tensor([1.9260, 1.6933], requires_grad=True)\n",
      "epoch 20:  tensor([1.9325, 1.7227], requires_grad=True)\n",
      "epoch 21:  tensor([1.9383, 1.7492], requires_grad=True)\n",
      "epoch 22:  tensor([1.9437, 1.7732], requires_grad=True)\n",
      "epoch 23:  tensor([1.9486, 1.7949], requires_grad=True)\n",
      "epoch 24:  tensor([1.9531, 1.8146], requires_grad=True)\n",
      "epoch 25:  tensor([1.9572, 1.8323], requires_grad=True)\n",
      "epoch 26:  tensor([1.9609, 1.8484], requires_grad=True)\n",
      "epoch 27:  tensor([1.9643, 1.8629], requires_grad=True)\n",
      "epoch 28:  tensor([1.9675, 1.8760], requires_grad=True)\n",
      "epoch 29:  tensor([1.9703, 1.8879], requires_grad=True)\n",
      "epoch 30:  tensor([1.9729, 1.8986], requires_grad=True)\n",
      "epoch 31:  tensor([1.9752, 1.9083], requires_grad=True)\n",
      "epoch 32:  tensor([1.9774, 1.9171], requires_grad=True)\n",
      "epoch 33:  tensor([1.9794, 1.9250], requires_grad=True)\n",
      "epoch 34:  tensor([1.9812, 1.9322], requires_grad=True)\n",
      "epoch 35:  tensor([1.9828, 1.9387], requires_grad=True)\n",
      "epoch 36:  tensor([1.9843, 1.9445], requires_grad=True)\n",
      "epoch 37:  tensor([1.9857, 1.9499], requires_grad=True)\n",
      "epoch 38:  tensor([1.9869, 1.9547], requires_grad=True)\n",
      "epoch 39:  tensor([1.9881, 1.9590], requires_grad=True)\n",
      "epoch 40:  tensor([1.9891, 1.9629], requires_grad=True)\n",
      "epoch 41:  tensor([1.9901, 1.9665], requires_grad=True)\n",
      "epoch 42:  tensor([1.9909, 1.9697], requires_grad=True)\n",
      "epoch 43:  tensor([1.9917, 1.9726], requires_grad=True)\n",
      "epoch 44:  tensor([1.9924, 1.9752], requires_grad=True)\n",
      "epoch 45:  tensor([1.9931, 1.9776], requires_grad=True)\n",
      "epoch 46:  tensor([1.9937, 1.9797], requires_grad=True)\n",
      "epoch 47:  tensor([1.9943, 1.9817], requires_grad=True)\n",
      "epoch 48:  tensor([1.9948, 1.9834], requires_grad=True)\n",
      "epoch 49:  tensor([1.9952, 1.9850], requires_grad=True)\n",
      "epoch 50:  tensor([1.9956, 1.9864], requires_grad=True)\n",
      "epoch 51:  tensor([1.9960, 1.9877], requires_grad=True)\n",
      "epoch 52:  tensor([1.9964, 1.9889], requires_grad=True)\n",
      "epoch 53:  tensor([1.9967, 1.9900], requires_grad=True)\n",
      "epoch 54:  tensor([1.9970, 1.9909], requires_grad=True)\n",
      "epoch 55:  tensor([1.9972, 1.9918], requires_grad=True)\n",
      "epoch 56:  tensor([1.9975, 1.9926], requires_grad=True)\n",
      "epoch 57:  tensor([1.9977, 1.9933], requires_grad=True)\n",
      "epoch 58:  tensor([1.9979, 1.9939], requires_grad=True)\n",
      "epoch 59:  tensor([1.9981, 1.9945], requires_grad=True)\n",
      "epoch 60:  tensor([1.9982, 1.9950], requires_grad=True)\n",
      "epoch 61:  tensor([1.9984, 1.9955], requires_grad=True)\n",
      "epoch 62:  tensor([1.9985, 1.9959], requires_grad=True)\n",
      "epoch 63:  tensor([1.9987, 1.9963], requires_grad=True)\n",
      "epoch 64:  tensor([1.9988, 1.9967], requires_grad=True)\n",
      "epoch 65:  tensor([1.9989, 1.9970], requires_grad=True)\n",
      "epoch 66:  tensor([1.9990, 1.9973], requires_grad=True)\n",
      "epoch 67:  tensor([1.9991, 1.9975], requires_grad=True)\n",
      "epoch 68:  tensor([1.9992, 1.9978], requires_grad=True)\n",
      "epoch 69:  tensor([1.9992, 1.9980], requires_grad=True)\n",
      "epoch 70:  tensor([1.9993, 1.9982], requires_grad=True)\n",
      "epoch 71:  tensor([1.9994, 1.9984], requires_grad=True)\n",
      "epoch 72:  tensor([1.9994, 1.9985], requires_grad=True)\n",
      "epoch 73:  tensor([1.9995, 1.9987], requires_grad=True)\n",
      "epoch 74:  tensor([1.9995, 1.9988], requires_grad=True)\n",
      "epoch 75:  tensor([1.9996, 1.9989], requires_grad=True)\n",
      "epoch 76:  tensor([1.9996, 1.9990], requires_grad=True)\n",
      "epoch 77:  tensor([1.9996, 1.9991], requires_grad=True)\n",
      "epoch 78:  tensor([1.9997, 1.9992], requires_grad=True)\n",
      "epoch 79:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 80:  tensor([1.9997, 1.9993], requires_grad=True)\n",
      "epoch 81:  tensor([1.9997, 1.9994], requires_grad=True)\n",
      "epoch 82:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 83:  tensor([1.9998, 1.9995], requires_grad=True)\n",
      "epoch 84:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 85:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 86:  tensor([1.9998, 1.9996], requires_grad=True)\n",
      "epoch 87:  tensor([1.9998, 1.9997], requires_grad=True)\n",
      "epoch 88:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 89:  tensor([1.9999, 1.9997], requires_grad=True)\n",
      "epoch 90:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 91:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 92:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 93:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 94:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 95:  tensor([1.9999, 1.9998], requires_grad=True)\n",
      "epoch 96:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 97:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 98:  tensor([1.9999, 1.9999], requires_grad=True)\n",
      "epoch 99:  tensor([1.9999, 1.9999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# this is my solution... asking ChatGPT isn't always the simplest solution :)\n",
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "lr = 0.001\n",
    "for epoch in range(100):\n",
    "    for data in dataset:\n",
    "        output = a_function(input_tensor=data, p=p)\n",
    "        l = loss(data, output)\n",
    "\n",
    "        p.grad = torch.zeros_like(p)\n",
    "        l.backward()\n",
    "        \n",
    "        p_grad = p.grad\n",
    "        p = p.detach()\n",
    "        p = p - lr*p_grad\n",
    "        p.requires_grad = True\n",
    "        \n",
    "\n",
    "    print(f'epoch {epoch}: ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 1:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 2:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 3:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 4:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 5:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 6:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 7:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 8:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 9:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 10:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 11:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 12:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 13:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 14:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 15:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 16:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 17:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 18:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 19:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 20:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 21:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 22:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 23:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 24:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 25:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 26:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 27:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 28:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 29:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 30:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 31:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 32:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 33:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 34:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 35:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 36:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 37:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 38:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 39:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 40:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 41:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 42:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 43:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 44:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 45:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 46:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 47:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 48:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 49:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 50:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 51:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 52:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 53:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 54:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 55:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 56:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 57:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 58:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 59:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 60:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 61:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 62:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 63:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 64:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 65:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 66:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 67:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 68:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 69:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 70:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 71:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 72:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 73:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 74:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 75:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 76:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 77:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 78:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 79:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 80:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 81:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 82:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 83:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 84:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 85:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 86:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 87:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 88:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 89:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 90:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 91:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 92:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 93:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 94:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 95:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 96:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 97:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 98:  tensor([ 1.5410, -0.2934], requires_grad=True)\n",
      "epoch 99:  tensor([ 1.5410, -0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# a possible WRONG way to do it\n",
    "torch.manual_seed(0)\n",
    "p = torch.randn([dim], requires_grad=True)\n",
    "lr = 0.001\n",
    "for epoch in range(100):\n",
    "    for data in dataset:\n",
    "        p_new = p.clone() # p.clone() creates a new tensor disconnected from the computation graph: p_new does not track gradients anymore.\n",
    "        output = a_function(input_tensor=data, p=p_new)\n",
    "        l = loss(data, output)\n",
    "\n",
    "        p.grad = torch.zeros([2])\n",
    "        l.backward()\n",
    "        p_new = p - lr*p.grad # The original p remains unchanged, and its .grad is never used to update it.\n",
    "        \n",
    "\n",
    "    print(f'epoch {epoch}: ', p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
