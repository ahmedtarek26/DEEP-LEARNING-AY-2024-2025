{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "1. Task\n",
    "2. Model\n",
    "3. Loss\n",
    "4. Training step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "real_params = torch.tensor([1.0, 3.0], requires_grad=False) # y = 1 + 3x\n",
    "\n",
    "params = torch.randn_like(real_params, requires_grad=True)\n",
    "print('params:', params)\n",
    "xaxis = torch.arange(-10, 10, 0.1)\n",
    "print(xaxis.size())\n",
    "yaxis = real_params[0] + real_params[1]*xaxis + torch.randn_like(xaxis)\n",
    "print(yaxis.size())\n",
    "dataset = torch.stack([xaxis, yaxis], dim=1)\n",
    "print(dataset.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(dataset[:,0], dataset[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(x_input: torch.Tensor, params: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    x_input: torch.Tensor\n",
    "    params: torch.Tensor\n",
    "\n",
    "    Output:\n",
    "    y_hat: torch.Tensor\n",
    "\n",
    "    x_input.shape:\n",
    "        - if x_input is a single data point: torch.Size([])\n",
    "        - if x_input is a batch of data points: torch.Size([batch_size])\n",
    "    \n",
    "    params.shape: torch.Size([2])\n",
    "    \n",
    "    y_hat.shape:\n",
    "        - if x_input is a single data point: torch.Size([])\n",
    "        - if x_input is a batch of data points: torch.Size([batch_size])\n",
    "    \"\"\"\n",
    "    y_hat = params[0] + params[1]*x_input\n",
    "    return y_hat\n",
    "\n",
    "def loss(y: torch.Tensor, y_hat: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    y: torch.Tensor\n",
    "    y_hat: torch.Tensor (y_hat = params[0] + params[1]*x_input)\n",
    "\n",
    "    Output:\n",
    "    loss: torch.Tensor\n",
    "\n",
    "    y.shape:\n",
    "        - if y is a single data point: torch.Size([])\n",
    "        - if y is a batch of data points: torch.Size([batch_size])\n",
    "        \n",
    "    y_hat.shape:\n",
    "        - if y_hat is a single data point: torch.Size([])\n",
    "        - if y_hat is a batch of data points: torch.Size([batch_size])\n",
    "\n",
    "    loss.shape: torch.Size([]) ALWAYS A SCALAR\n",
    "    \"\"\"\n",
    "    return (y - y_hat).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "params = torch.randn_like(real_params, requires_grad=True)\n",
    "lr = 0.001\n",
    "opt = torch.optim.SGD([params], lr=lr)\n",
    "\n",
    "params_history_single = []\n",
    "loss_history_single = []\n",
    "\n",
    "n_updates = 0\n",
    "for epoch in range(30):\n",
    "    for i, datapoint in enumerate(dataset):\n",
    "        y_hat = regression(datapoint[0], params)\n",
    "        l = loss(datapoint[1], y_hat)\n",
    "        #print(datapoint[0].shape, y_hat.shape, datapoint[1].shape)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        n_updates += 1\n",
    "    \n",
    "        \n",
    "    params_history_single.append(params.clone().detach().numpy())\n",
    "    loss_history_single.append(l.clone().detach().numpy())\n",
    "\n",
    "params_history_single = np.array(params_history_single)\n",
    "loss_history_single = np.array(loss_history_single)\n",
    "\n",
    "print('params:', params)\n",
    "print('n_updates:', n_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "params = torch.randn_like(real_params, requires_grad=True)\n",
    "lr = 0.001\n",
    "opt = torch.optim.SGD([params], lr=lr)\n",
    "\n",
    "params_history_batch = []\n",
    "loss_history_batch = []\n",
    "\n",
    "N = len(dataset) # 200\n",
    "num_batches = 20\n",
    "batch_size = N // num_batches\n",
    "print('batch_size:', batch_size)\n",
    "\n",
    "# dataset = [[data1,... data10], [data11...data20], ....]\n",
    "\n",
    "n_updates = 0\n",
    "for epoch in range(30):\n",
    "    for i in range(num_batches):\n",
    "        start = i*batch_size\n",
    "        end = i*batch_size+batch_size\n",
    "        y_hat = regression(dataset[start:end,0], params)\n",
    "        l = loss(dataset[start:end,1], y_hat)\n",
    "        #print(dataset[i:i+batch_size,0].shape, y_hat.shape, dataset[i:i+batch_size,1].shape)\n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        n_updates += 1\n",
    "    \n",
    "\n",
    "    params_history_batch.append(params.clone().detach().numpy())\n",
    "    loss_history_batch.append(l.clone().detach().numpy())\n",
    "\n",
    "print('params:', params)\n",
    "print('n_updates:', n_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "params = torch.randn_like(real_params, requires_grad=True)\n",
    "lr = 0.001\n",
    "opt = torch.optim.SGD([params], lr=lr)\n",
    "\n",
    "params_history_total = []\n",
    "loss_history_total = []\n",
    "\n",
    "n_updates = 0\n",
    "for epoch in range(30):\n",
    "    y_hat = regression(dataset[:,0], params)\n",
    "    l = loss(dataset[:,1], y_hat)  \n",
    "    #print(dataset[:,0].shape, y_hat.shape, dataset[:,1].shape)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    params_history_total.append(params.detach().clone().numpy())\n",
    "    loss_history_total.append(l.detach().numpy())\n",
    "    n_updates += 1\n",
    "\n",
    "print('params:', params)\n",
    "print('n_updates:', n_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "true_p0 = 1.0\n",
    "true_p1 = 3.0\n",
    "\n",
    "# y = p0 + p1*x\n",
    "x = torch.tensor([-1.0, 1.0])\n",
    "y = true_p0 + true_p1 * x  # y = [-2, 4]\n",
    "\n",
    "def loss_p(p0, p1):\n",
    "    return (y - (p0 + p1 * x)).pow(2).mean()\n",
    "\n",
    "# Narrow the range around the optimum (1, 3)\n",
    "p0_vals = np.linspace(0, 2, 100)\n",
    "p1_vals = np.linspace(2, 4, 100)\n",
    "p0_mesh, p1_mesh = np.meshgrid(p0_vals, p1_vals)\n",
    "\n",
    "loss_vals = np.zeros_like(p0_mesh)\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        loss_vals[i, j] = loss_p(p0_mesh[i, j], p1_mesh[i, j])\n",
    "\n",
    "# 3D plot of the loss surface\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(p0_mesh, p1_mesh, loss_vals, cmap='viridis')\n",
    "ax.set_xlabel('p0')\n",
    "ax.set_ylabel('p1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(params_history_batch, loss_history_batch):\n",
    "    # Ground truth parameters\n",
    "    true_p0, true_p1 = 1.0, 3.0\n",
    "\n",
    "    # Generate the simple dataset\n",
    "    x = torch.tensor([-1.0, 1.0])\n",
    "    y = true_p0 + true_p1 * x \n",
    "\n",
    "    # Loss function\n",
    "    def loss_p(p0, p1):\n",
    "        return (y - (p0 + p1 * x)).pow(2).mean()\n",
    "\n",
    "    # Generate mesh grid around optimal parameters\n",
    "    p0_vals = np.linspace(0, 2, 100)\n",
    "    p1_vals = np.linspace(2, 4, 100)\n",
    "    p0_mesh, p1_mesh = np.meshgrid(p0_vals, p1_vals)\n",
    "\n",
    "    # Compute loss values for the mesh grid\n",
    "    loss_vals = np.zeros_like(p0_mesh)\n",
    "    for i in range(p0_mesh.shape[0]):\n",
    "        for j in range(p0_mesh.shape[1]):\n",
    "            loss_vals[i, j] = loss_p(p0_mesh[i, j], p1_mesh[i, j])\n",
    "\n",
    "    # Convert param history to numpy for plotting\n",
    "    params_history_batch = np.array(params_history_batch)\n",
    "    loss_history_batch = np.array(loss_history_batch)\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the loss surface with transparency for visibility\n",
    "    surf = ax.plot_surface(p0_mesh, p1_mesh, loss_vals, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "\n",
    "    # Overlay optimization trajectory clearly\n",
    "    ax.plot(\n",
    "        params_history_batch[:, 0],\n",
    "        params_history_batch[:, 1],\n",
    "        loss_history_batch,\n",
    "        color='red', linewidth=3, marker='o', markersize=5, label='Optimization Path'\n",
    "    )\n",
    "\n",
    "    # Highlight the initial and final points clearly\n",
    "    ax.scatter(params_history_batch[0, 0], params_history_batch[0, 1], loss_history_batch[0], color='blue', s=80, label='Start', edgecolor='black')\n",
    "    ax.scatter(params_history_batch[-1, 0], params_history_batch[-1, 1], loss_history_batch[-1], color='yellow', s=80, label='End', edgecolor='black')\n",
    "\n",
    "    # Axes labels and legend\n",
    "    ax.set_xlabel('p0', fontsize=12)\n",
    "    ax.set_ylabel('p1', fontsize=12)\n",
    "    \n",
    "   \n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.title(\"Clear 3D Loss Surface with Optimization Path\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(params_history_single, loss_history_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(params_history_batch[2:], loss_history_batch[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(params_history_total[2:], loss_history_total[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.utils.Dataset`, `Dataloaders` and `torchvision.datasets`\n",
    "\n",
    "`Dataset` provides an interface to access individual data points and defining your own dataset class.\n",
    "\n",
    "`DataLoader` is an iterator which provides all these features like batching, shuffling, multi-process data loading and many more. It is defined starting from a `Dataset` object.\n",
    "\n",
    "`torchvision.datasets` provides some common datasets like MNIST, CIFAR10, etc.\n",
    "\n",
    "The `Dataset` is an abstraction to be able to load and process each sample of your dataset lazily, while the `DataLoader` takes care of shuffling/sampling/weigthed sampling, batching, using multiprocessing to load the data, use pinned memory etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(\"~/datasets/\", train=True, download=True,\n",
    "                        transform=transforms.ToTensor()\n",
    "                        )\n",
    "\n",
    "print(train_dataset)\n",
    "print(train_dataset.data.shape)\n",
    "print(train_dataset.targets.shape)\n",
    "print(train_dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = next(iter(train_dataloader))\n",
    "print(iteration[0].shape)\n",
    "print(iteration[1].shape)\n",
    "print(iteration[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
