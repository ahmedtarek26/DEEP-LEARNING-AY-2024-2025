{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: backpropagation and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. .grad\n",
    "2. .backword()\n",
    "3. retain_grad(),retain_graph\n",
    "4. torch.optim, .stop()\n",
    "\n",
    "Examples:\n",
    "1. scaler example\n",
    "2. vector example\n",
    "3. function example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch introduction\n",
    "\n",
    "Pytorch python library that provided tools to work with tensors.\n",
    "Feature: allows for Trcking gradient odf tensors\n",
    "\n",
    "Tensors: a multidimentional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor\n",
    "\n",
    "my_tensor = torch.tensor([[1.0,2.0],[1.0,2.0]])\n",
    "my_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple scalar example\n",
    " \n",
    "Let's define the following: $\\\\ p \\in \\mathbb{R} \\\\ w = 10p \\\\ l = w^2$.\n",
    " \n",
    "From calculus we know that:\n",
    "$$  \\frac{\\partial w}{\\partial p} = 10, \\\n",
    "\\frac{\\partial l}{\\partial w} = 2w, \\\n",
    "\\frac{\\partial l}{\\partial p} = \\frac{\\partial l}{\\partial w} \\frac{\\partial w}{\\partial p} = 2w * 10 = 2(10p)*10 = 200p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that p is @free@ (it doesn't depend on anything)\n",
    "\n",
    "l -- > w --> p\n",
    "\n",
    "Actually, torch is building this kind of graph when I'll define thoes tensors. This graph that toech build is called computational graph.\n",
    "Tensor that do not depend in anything (p) are called leaf node (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True) tensor([10.], grad_fn=<MulBackward0>) tensor([100.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def init_variables(scaler: float=1.0, requires_grad: bool=True):\n",
    "    p = torch.tensor([scaler],requires_grad=requires_grad)\n",
    "    w = 10*p \n",
    "    l= w**2 # pow(2)\n",
    "    return p, w, l\n",
    "\n",
    "p,w,l=init_variables(1.0)\n",
    "print(p,w,l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each torch has a requires_grid attribute. this attr allows for tracking gradients for the tensor --> if a tensor has requrs_grad=True, it'll be attached to computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s0 ...how to compute derivatives??\n",
    "\n",
    "In torch, each tensor having requires_grad=True, has a method called .backwards(), which computes the derivative of that tensor wrt the leaf nodes.\n",
    "\n",
    "tensor.backwords()\n",
    "\n",
    "w.backwords()\n",
    "\n",
    "## Graph leaves are alwayes the tensors will compute the gradients with respect to when calling BACKWARD()\n",
    "\n",
    "bUT WHERE THE RESULT IS SORTED?\n",
    "in torch, when yo call .backward(), the result of computational is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p,w,_=init_variables(scaler=2.0)\n",
    "p.backward()\n",
    "p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18869/406291535.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "p,w,_=init_variables(scaler=2.0)\n",
    "w.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18869/2209070905.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(w.grad)\n"
     ]
    }
   ],
   "source": [
    "def init_variables_retain_w(scaler: float=1.0, requires_grad: bool=True):\n",
    "    p = torch.tensor([scaler],requires_grad=requires_grad)\n",
    "    w = 10*p \n",
    "    l= w**2 # pow(2)\n",
    "    return p, w, l\n",
    "p,w,l=init_variables_retain_w(1.0)\n",
    "w.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([200.])\n"
     ]
    }
   ],
   "source": [
    "p,w,l=init_variables(scaler=1.0)\n",
    "l.backward()\n",
    "\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl / dw\n",
    "p ,w, l = init_variables_retain_w(scaler=1.0)\n",
    "l.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# dl / dp =  dl /dw * dw / dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "p ,w, l = init_variables_retain_w(scaler=1.0)\n",
    "w.backward(retain_graph=True)\n",
    "print(p.grad)\n",
    "# computational graph doesn't exist anymore.\n",
    "#p.gard = torch.zeros_like(p)\n",
    "#p.grad = torch.zero([1])\n",
    "p.grad.zero_()\n",
    "w.backward()\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clone and deattach\n",
    "\n",
    "clone: makes a copy of the tensor it is called on ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with tensors\n",
    " \n",
    "Let's define the following: $\\\\ p = (2,2,2,2) \\\\ w = p^2$ \\\n",
    "and a  function $$ \\ell(p,w) = \\sum_{i=1}^{4} (p_i - w_i)^2 = \\sum_{i=1}^{4} (p_i - p_i^2)^2 = \\ell(p). $$\n",
    "Hence $$ \\frac{\\partial \\ell}{\\partial p_i} = 2(p_i-p_i^2)(1-2p_i) = 4p_i^3-6p_i^2+2p_i $$\n",
    " \n",
    "In particular:\n",
    "$$ \\nabla_p \\ell = (\\frac{\\partial \\ell}{\\partial p_1}, \\frac{\\partial \\ell}{\\partial p_2}, \\frac{\\partial \\ell}{\\partial p_3}, \\frac{\\partial \\ell}{\\partial p_4}) = (4p_1^3-6p_1^2+2p_1, 4p_2^3-6p_2^2+2p_2, 4p_3^3-6p_3^2+2p_3, 4p_4^3-6p_4^2+2p_4)$$\n",
    " \n",
    "Note also taht:\n",
    "$$ \\frac{\\partial \\ell}{\\partial w_i} = -2(p_i-w_i) = -2(p_i-p_i^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2.], requires_grad=True) tensor([4., 4., 4., 4.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "p = torch.tensor([2.0,2.0,2.0,2.0],requires_grad=True)\n",
    "w = p**2\n",
    "print(p,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<SumBackward0>)\n",
      "tensor([12., 12., 12., 12.])\n"
     ]
    }
   ],
   "source": [
    "def l_fn(p,w) -> torch.Tensor:\n",
    "    return (p-w).pow(2).sum()\n",
    "\n",
    "p= torch.tensor([2.0,2.0,2.0,2.0],requires_grad=True)\n",
    "w = p**2\n",
    "l = l_fn(p,w)\n",
    "print(l)\n",
    "l.backward()  # dl / dp, result is 2*(p-w) = 2*(2-4) = -4\n",
    "\n",
    "print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Far...\n",
    "\n",
    "1. we can compute derivatives of scaler (l) wrt to leaf nodes (p) (l.backward())\n",
    "2. the result will be in p.grad\n",
    "\n",
    "Next step: how to change p according to p.grad?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
